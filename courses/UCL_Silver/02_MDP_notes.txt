Markov Decision Process:

Markov Reward Process: A Markov Reward Process is a tuple \begin{equation} \langle\mathcal{S}, \mathcal{T}, \mathcal{R}, \gamma\rangle \end{equation}

Return (\(G_{t}\)): the total discounted reward from time-step t.

\begin{equation}
G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
\end{equation}

A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov
\begin{equation}
\langle\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma\rangle
\end{equation}


Policy: 
A policy $ \pi $ is a distribution over actions given states
(fully defines the behaviour of an agent) \&\& stationary (time-independent) \begin{equation} A_{t} \sim \pi\left(\cdot | S_{t}\right), \forall t>0 \end{equation}
\begin{equation}
\pi(a | s)=\mathbb{P}\left[A_{t}=a | S_{t}=s\right]
\end{equation}

Value Function:

State-Value function: the expected return starting from state s, and then following policy $ \pi $
\begin{equation}
v_{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right]
\end{equation}

action-value function: the expected return starting from state s, \textit{taking action a}, and then following policy $ \pi $
\begin{equation}
q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[G_{t} | S_{t}=s, A_{t}=a\right]
\end{equation}


**Optimal Value Function**:
get maximum over all policies

Optimal Policy: 
An optimal policy can be found by maximising over $ q_{*}(s, a) $
\begin{equation}
\pi_{*}(a | s)=\left\{\begin{array}{ll}{1} \& {\text { if } a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} q_{*}(s, a)} \\ {0} \& {\text { otherwise }}\end{array}\right.
\end{equation}

Bellman Optimality Equation:
\begin{equation}
v_{*}(s)=\max _{a} \mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{T}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)
\end{equation}

\begin{equation}
q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{T}_{s s^{\prime}}^{a} \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)
\end{equation}