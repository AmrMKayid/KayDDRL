# Algorithm:

## Asynchronous Actor-Critic Agents (**A3C**) | 2016 Baseline Algo:
faster, simpler and more robust than the DQN as it depends on using multiple workers.


| DQN | A3C |
| :---:| :---: | 
| single agent/worker | multiple workers |
| single env to generate training data | each worker has it's own env to interact with |
| sequential | Asynchronous, train Net -> Share results at the end of sim |

### Advantages:

- Asynchronous
- More workers -> More **DATA**
- Data diversity from diff. env. => result will be more robust and general

### Articles: [idea](https://sergioskar.github.io/Actor_critics/) | [Doom game](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2)

### New Verions:
- **[GA3C](https://openreview.net/forum?id=r1VGvBcxl)**: using GPU to to store the network globally instead of copying it to each worker
- **[Distributed BA3c](https://arxiv.org/abs/1801.02852)**: trained efficiently on a large cluster of 64 12-core CPUs => 21 minutes to play Atari games! [github](https://github.com/deepsense-ai/Distributed-BA3C)
- [Rainbow](): Combining Improvements in Deep Reinforcement Learning

<!-- ############################# -->

## Distributed Prioritized Experience Replay (**Ape-X**) | 2018:

- Scalable more than A3C
- Distributed applied to -> DQN/DDPG
- Reintroduce Experience Replay
- generate large quantities of data in parallel.

<!-- ############################# -->

## **IMPALA**: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures | 2018:

- principled off-policy advantage actor critic **(V-trace)**
- independent actors and learners -> increasing the throughput
- collect experience which is passed to a central learner that computes gradients
- maximises data throughput using an efficient distributed architecture

<!-- ########################################################## -->

# Architecture:

## Ray & Rllib:

- Scalable execution of algorithms based on Ray
- High performance and substantial code reuse
- Easily compare and reproduce algorithms


## CPU/GPUs:

- Singe Machine at first or Colab/AWS
- number of CPUs/GPUs available?
- Cluster Computers?


## Env:

- All seen envs are from openai gym or deepmind lab
- Share same abstractions across all env
- states and actions in nrp?!
- rendering!

## Benchmarking:

Most of research benchmark on **Atari2600** 