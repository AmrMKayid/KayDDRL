% !TeX root = ../../main.tex

\subsection{2-DOF Arm Gym Base Non-Distributed Experiment}

This experiment is the \textit{base experiment} of a set of experiments for reinforcement learning in robots simulations. In this way, we perform an initial experiment with a selected reinforcement learning algorithm on the 2-DOF robotic arm to achieve the goal of the environment and reach the target as quickly and efficiently as possible. Based on this experiment, we plan to build on it more advanced experiments to show the effect of using distribution for reinforcement learning tasks and the benefit of parallelizing the environments to enhance the performance of the agents and solve the task quickly. Performing the experiment, we plan to extend it to a more complex environment with modified reward function and on different physics engine to compare the existing reinforcement learning platforms and express the difference in comparison. Hence, we study the effect of distribution and transferability between the different engine and the efficiency between them.

\subsubsection{Aim of the experiment}

This experiment is designed to be performed on non-distributed setup using only the power of CPUs, which make it the base experiment for the performed experiments and to be able to compare between different selected reinforcement learning methods and algorithms and show the effect of using distributed algorithms and parallelizing experiment's environment to achieve the efficiency and speed wanted to perform reinforcement learning experiments. 

We want to investigate how the agent will perform in the experiment, the time taken to run the experiment, the average episode reward the agent will get and whether the agent will be able to solve the environment in constrained stopping conditions.

\subsubsection{Setup and configurations}

In this section we describe the setup of the experiment and how it was performed. Firstly, we introduce and describe the RoboReacher robotics arm environment provided by OpenAI Gym and PyBullet physics simulator. Then, based on the environment description, we present the observation space, action space and reward function of the experiment as a base towards the learning process and achieving environment goal. Subsequently, we describe the learning process. For this, we present the reinforcement learning algorithm and neural network architecture used.


\subsubsection{Environment Description: 2-DOF Robot Arm | Roboschool Reacher}
Roboschool is an open-source software for robot simulation, integrated with OpenAI Gym. The selected environment is \textit{Reacher Environment}~\ref{fig:openai_reacher}: A robotic arm consist of two linked joints places in a squared arena surrounding it along with a moving sphere (target). The goal of the robotics arm it to reach target sphere and maintain following the point until the end of the episode. 

\begin{figure}[!htb]
		\centering
		\includegraphics[width=0.7\linewidth]{figures/envs/openai_roboreacher.png}
		\caption{OpenAI Reacher Environment}
		\label{fig:openai_reacher}
\end{figure}

\subsubsection{Observation Space}

The observation space of the environment, shown in the table~\ref{tab:gym_reacher_obs} below, consist of \textbf{9 variables} corresponding to the position of the target sphere, the x-axis and y-axis components of the vector from the target to the fingertip, cos(theta) and sin(theta) for the joints and the angular velocity of the fingertip in the x and y directions.

\begin{table}[!htb]
		\centering
		\begin{tabular}{|c|c|}
				\hline
				\multicolumn{2}{|c|}{{\ul \textit{\textbf{Observation Space}}}}                                                                                   \\ \hline
				\multirow{2}{*}{\textbf{Target Position}}                                                                      & \textit{X Position}              \\ \cline{2-2} 
																																																								& \textit{Y Position}              \\ \hline
				\multirow{2}{*}{\textbf{Arm to Target Vector}}                                                                 & \textit{Position vector 0}       \\ \cline{2-2} 
																																																								& \textit{Position vector 1}       \\ \hline
				\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Current Relative Position\\ of Central Joint\end{tabular}}} & \textit{cosine of central joint} \\ \cline{2-2} 
						& \textit{sine of central joint}   \\ \hline
				\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Current Relative Position\\ of Elbow Joint\end{tabular}}}   & \textit{cosine of elbow joint}   \\ \cline{2-2} 
						& \textit{sine of elbow joint}     \\ \hline
		\end{tabular}
		\caption{Gym Reacher Observation Information}
		\label{tab:gym_reacher_obs}
\end{table}

\subsubsection{Action Space}

The action space of the environment, shown in the table~\ref{tab:gym_reacher_actions} below, is a continuous one which indicated the torque applied on both of the robotic arm joints.

\begin{table}[!htb]
		\centering
		\begin{tabular}{|c|c|}
				\hline
				\multicolumn{2}{|c|}{{\ul \textit{\textbf{Action Space (Continuous)}}}}                             \\ \hline
				\multirow{2}{*}{\textbf{Center Joint Torque}} & \multirow{2}{*}{\textit{range(-1, 1)}} \\
																												&                                        \\ \hline
				\multirow{2}{*}{\textbf{Elbow Joint Torque}}  & \multirow{2}{*}{\textit{range(-1, 1)}} \\
																												&                                        \\ \hline
		\end{tabular}
				\caption{Gym Reacher Action Information}
				\label{tab:gym_reacher_actions}
\end{table}

\subsubsection{Reward Function}

\let\realurl\url
\renewcommand{\url}[1]{%
  \realurl{#1}%
  \wlog{URLX #1 }%
}

The reward function\footnote{\url{https://github.com/openai/roboschool/blob/master/roboschool/gym_reacher.py\#L70}} is designed based on the distance between the arm and the target along with electricity cost of the torque and angular velocity of the arm with small epsilon amount in case of the joint is stuck. 


\subsubsection{Algorithm}

In this experiment, \textbf{Proximal Policy Optimization (PPO)} algorithm is selected to be the base algorithm for our initial experiment. 

Below are the default configuration for PPO algorithm:
\lstinputlisting[language=Python]{chapters/04/ppo_default.py}

followed by this experiment setup and configuration as listed below:
\begin{table}[!htb]
		\centering
		\begin{tabular}{|c|l|l|c|l|l|}
				\hline
				\multicolumn{6}{|c|}{\textit{\textbf{Gym Reacher PPO 1st Experiment: Non-Distributed Experiment}}}                                                        \\ \hline
				\multicolumn{3}{|c|}{\textbf{env}}                                  & \multicolumn{3}{c|}{RoboschoolReacher-v1}                                           \\ \hline
				\multicolumn{3}{|c|}{\textbf{env\_type}}                            & \multicolumn{3}{c|}{OpenAI Environment}                                             \\ \hline
				\multicolumn{3}{|c|}{\textbf{run: Algorithms}}                      & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{PPO}}                           \\ \hline
				\multicolumn{3}{|c|}{}                                              & \multicolumn{3}{c|}{\cellcolor[HTML]{E1F7E1}episode\_reward\_mean = 21}             \\ \cline{4-6} 
				\multicolumn{3}{|c|}{\multirow{-2}{*}{\textbf{stop condition}}}     & \multicolumn{3}{c|}{\cellcolor[HTML]{E1F7E1}time-steps\_total = 10000000: 10M Steps} \\ \hline
				\multicolumn{3}{|c|}{\textbf{gamma}}                                & \multicolumn{3}{c|}{0.99}                                                           \\ \hline
				\multicolumn{3}{|c|}{\textbf{kl coefficient}}                            & \multicolumn{3}{c|}{1.0}                                                            \\ \hline
				\multicolumn{3}{|c|}{\textbf{num\_sgd\_iter}}                       & \multicolumn{3}{c|}{20}                                                             \\ \hline
				\multicolumn{3}{|c|}{\textbf{lr}}                                   & \multicolumn{3}{c|}{0.0001}                                                         \\ \hline
				\multicolumn{3}{|c|}{\textbf{sgd\_minibatch\_size}}                 & \multicolumn{3}{c|}{1000}                                                           \\ \hline
				\multicolumn{3}{|c|}{\textbf{train\_batch\_size}}                   & \multicolumn{3}{c|}{25000}                                                          \\ \hline
				\multicolumn{3}{|c|}{\textbf{batch\_mode}}                          & \multicolumn{3}{c|}{complete\_episodes}                                             \\ \hline
				\multicolumn{3}{|c|}{\textbf{observation\_filter}}                  & \multicolumn{3}{c|}{MeanStdFilter}                                                  \\ \hline
				\multicolumn{3}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{num\_gpus}}    & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}0}                                      \\ \hline
				\multicolumn{3}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{num\_workers}} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}0}                                      \\ \hline
		\end{tabular}
		\caption{Gym Reacher PPO 1st Experiment: Non-Distributed Experiment}
		\label{tab:gym_reacher_ppo_1st_exp}
\end{table}


\subsubsection{Neural Network Architecture}
In this simple environment, the observation space is not a visual (pixel raw images) observation, which means it won't require a convolutional neural network to process it, Hence, the algorithm will be using fully connected neural network consist of input layer of 9 variables for the observation, two hidden layers with 256 neuron for each layer, and the output layer of 2 neurons corresponding to the actions send to the environment as shown below~\ref{fig:ppo_nn}:

\begin{figure}[!htb]
		\centering
		\includegraphics[width=\linewidth]{figures/exps/1st_exp/ppo_nn}
		\caption{PPO Neural Network Architecture}
		\label{fig:ppo_nn}
\end{figure}


\subsubsection{Experiment Results}

After performing the experiment with a stopping conditions either \textit{\textbf{reaching an average reward of 21 or total time steps of the agent is 10M steps}} as indicated in ~\ref{tab:gym_reacher_ppo_1st_exp}. We measure the environment and training process to evaluate the experiment and be able to compare with others.

We start by measuring the maximum reward~\ref{fig:1st_exp_max_eps_reward} the agent could get from the environment, as shown in the figure below, the maximum the agent could obtain is 30 reward over all the training time-steps it has performed.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=\linewidth]{figures/exps/1st_exp/max_eps_reward}
	\caption{Maximum Reward over Episodes}
	\label{fig:1st_exp_max_eps_reward}
\end{figure}

Followed by measuring what is the minimum reward the agent is getting, shown in the figure~\ref{fig:1st_exp_min_eps_reward}, and the observation shows that the agent is stuck under -30 reward and it's performance has high variance before completing 2M time-steps.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/exps/1st_exp/min_eps_reward}
	\caption{Minimum Reward over Episodes}
	\label{fig:1st_exp_min_eps_reward}
\end{figure}
The experiment was constrained under the conditions of reaching average reward of 21 or completing 10M steps. Based on the following figure~\ref{fig:1st_exp_avg_eps_reward}, the agent was not able to obtain the required reward for the first stopping condition. Hence, the agent completed the 10M time-steps without achieving the goal of the environment. This indicated the bad performance for this agent (getting only +1 reward) and the training process in a non-distributed environment wasn't sufficient to complete the required task.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/exps/1st_exp/avg_eps_reward}
	\caption{Average Reward over Episodes}
	\label{fig:1st_exp_avg_eps_reward}
\end{figure}
The following measurement~\ref{fig:1st_exp_total_loss} describe the total loss of the algorithm which shows high variance and indicate that the algorithm needed more time to reduce the loss and reach a minimum better than this to enhance the agent performance.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/exps/1st_exp/total_loss}
	\caption{Total Loss}
	\label{fig:1st_exp_total_loss}
\end{figure}
The most important metric for the experiment, which is the aim we want to reduce as much as possible, is the \textbf{total training time} for the experiment. The following figure shows the training time for the experiment in hours. This indicated how much time needed to perform such an experiment. From the figure~\ref{fig:1st_exp_total_training_time}, the experiment time is approximately \textbf{4 Hours}, which is very huge for performing normal task in a simple robotic environment.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{figures/exps/1st_exp/total_time}
	\caption{Total Training Time}
	\label{fig:1st_exp_total_training_time}
\end{figure}

\subsubsection{Conclusion}

In this experiment, we perform a training and evaluation for the simple robotic task using ppo algorithm in non-distributed setup to train our agent. we conclude that the experiment took a large amount of time to train the agent (4 hours) and at the end the agent didn't solve the required task and the learning process was not successful. 

Summary for the results:
\begin{itemize}
	\item The agent completed the 10M steps.
	\item The agent could not solve the required task.
	\item The average the agent could get did not exceed 1 reward over the whole episodes~\ref{fig:1st_exp_avg_eps_reward}.
	\item The maximum the agent could get did not exceed 30 reward over the whole episodes~\ref{fig:1st_exp_max_eps_reward}.
	\item The performance of the agent is not quite good.
	\item The total loss is not improving and didn't reach global minimum~\ref{fig:1st_exp_total_loss}.
	\item The total time elapsed exceed 3.5 hours for such a simple environment and task~\ref{fig:1st_exp_total_training_time}.
\end{itemize}