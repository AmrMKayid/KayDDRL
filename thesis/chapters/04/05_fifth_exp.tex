% !TeX root = ../../main.tex

\subsection{Transferring from Gym to Unity 2-DOF Base Environment}

In this experiment, we create a similar environment in Unity to test the transferability between different two physics engine. This experiment is conducted to test the trained agent on a new different environment which has the same observation and action spaces like the environment that was used for training with a different reward function. In this way, we will be using the trained agent from the third experiment to test it on this experiment and observe the performance and behaviour of the agent.

\subsubsection{Setup and configurations}

In this section we describe the setup of the experiment and how it was performed. Firstly, we introduce and describe the RoboReacher robotics arm environment provided by OpenAI Gym and PyBullet physics simulator. Then, based on the environment description, we present the observation space, action space and reward function of the experiment as a base towards the learning process and achieving environment goal. Subsequently, we describe the learning process. For this, we present the reinforcement learning algorithm and neural network architecture used.


\subsubsection{Environment Description}
Using Unity physics engine and Unity ML-Agent, we have built a new 2-DOF robotic environment~\ref{fig:unity_reacher} which is similar to OpenAI Gym environment. The environment has the same observation and action spaces like gym environment and we have modified the reward function to be more complex in this environment to test the behavior of the trained agent.

\begin{figure}[!htb]
		\centering
				\includegraphics[width=0.7\linewidth]{figures/envs/unity_roboreacher.png}
				\caption{Unity 2-DOF Environment}
				\label{fig:unity_reacher}
\end{figure}

\subsubsection{Reward Function}

The reward function for this environment is different as we modify it to only give a reward of (+0.1) for the agent if the agent can reach the target sphere and keep moving with it along a circular path.


\subsubsection{Experiment Results}

Since this is an evaluation experiment where we have the trained agent from the 3rd experiment, the evaluation will be based on the average reward the agent can obtain after running for 1M time-steps. In this experiment, the agent was able to obtain an average reward ranging from 15 to 20 over the whole testing time-steps as shown in the figure below.

\begin{figure}[!htb]
		\centering
				\includegraphics[width=\linewidth]{figures/exps/unity2d.png}
				\caption{Unity 2-DOF Average Reward}
				\label{fig:unity2d_avg_reward}
\end{figure}


\subsubsection{Conclusion}

In this experiment, we perform a training and evaluation for the simple robotic task using ppo algorithm in non-distributed setup to train our agent. we conclude that the experiment took a large amount of time to train the agent (4 hours) and at the end the agent didn't solve the required task and the learning process was not successful. 